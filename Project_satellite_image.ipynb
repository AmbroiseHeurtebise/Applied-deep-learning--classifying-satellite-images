{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-03-28T01:58:36.651461Z",
     "iopub.status.busy": "2022-03-28T01:58:36.651189Z",
     "iopub.status.idle": "2022-03-28T01:58:36.664054Z",
     "shell.execute_reply": "2022-03-28T01:58:36.663309Z",
     "shell.execute_reply.started": "2022-03-28T01:58:36.651428Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('../input/'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains satellite images covering different landscapes like rural areas, urban areas, densely forested, mountainous terrain, small to large water bodies, agricultural areas, etc. covering the whole state of California. It is avaibable here : https://www.kaggle.com/crawford/deepsat-sat6?select=sat-6-full.mat.\n",
    "\n",
    "The images are 28x28 pixels with 4 bands - red, green, blue and near infrared. We take 100.000 of such images for the train set and 50.000 for test set. Finally, the training and test labels are one-hot encoded 1x6 vectors for the 6 different classes of landscapes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T01:58:38.239658Z",
     "iopub.status.busy": "2022-03-28T01:58:38.238860Z",
     "iopub.status.idle": "2022-03-28T01:58:38.941314Z",
     "shell.execute_reply": "2022-03-28T01:58:38.940523Z",
     "shell.execute_reply.started": "2022-03-28T01:58:38.239593Z"
    }
   },
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T01:58:38.945091Z",
     "iopub.status.busy": "2022-03-28T01:58:38.944867Z",
     "iopub.status.idle": "2022-03-28T01:58:38.949101Z",
     "shell.execute_reply": "2022-03-28T01:58:38.948428Z",
     "shell.execute_reply.started": "2022-03-28T01:58:38.945066Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data_path=\"../input/deepsat-sat6/X_train_sat6.csv\"\n",
    "train_label_path=\"../input/deepsat-sat6/y_train_sat6.csv\"\n",
    "test_data_path=\"../input/deepsat-sat6/X_test_sat6.csv\"\n",
    "test_lable_path=\"../input/deepsat-sat6/y_test_sat6.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path=\"archive/X_train_sat6.csv\"\n",
    "train_label_path=\"archive/y_train_sat6.csv\"\n",
    "test_data_path=\"archive/X_test_sat6.csv\"\n",
    "test_lable_path=\"archive/y_test_sat6.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T01:58:39.462066Z",
     "iopub.status.busy": "2022-03-28T01:58:39.461522Z",
     "iopub.status.idle": "2022-03-28T01:58:39.466297Z",
     "shell.execute_reply": "2022-03-28T01:58:39.465560Z",
     "shell.execute_reply.started": "2022-03-28T01:58:39.462028Z"
    }
   },
   "outputs": [],
   "source": [
    "def data_read(data_path, nrows):\n",
    "    data=pd.read_csv(data_path, header=None, nrows=nrows, dtype=np.uint8)\n",
    "    data=data.values ## converting the data into numpy array\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T01:58:40.145455Z",
     "iopub.status.busy": "2022-03-28T01:58:40.145201Z",
     "iopub.status.idle": "2022-03-28T01:59:52.588184Z",
     "shell.execute_reply": "2022-03-28T01:59:52.587341Z",
     "shell.execute_reply.started": "2022-03-28T01:58:40.145426Z"
    }
   },
   "outputs": [],
   "source": [
    "##Read training data\n",
    "train_data=data_read(train_data_path, nrows=100000)\n",
    "print(\"Train data shape:\" + str(train_data.shape))\n",
    "\n",
    "##Read training data labels\n",
    "train_data_label=data_read(train_label_path,nrows=100000)\n",
    "print(\"Train data label shape:\" + str(train_data_label.shape))\n",
    "print()\n",
    "\n",
    "##Read test data\n",
    "test_data=data_read(test_data_path, nrows=50000)\n",
    "print(\"Test data shape:\" + str(test_data.shape))\n",
    "\n",
    "\n",
    "##Read test data labels\n",
    "test_data_label=data_read(test_lable_path,nrows=50000)\n",
    "print(\"Test data label shape:\" + str(test_data_label.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T01:59:52.590229Z",
     "iopub.status.busy": "2022-03-28T01:59:52.589976Z",
     "iopub.status.idle": "2022-03-28T01:59:52.598440Z",
     "shell.execute_reply": "2022-03-28T01:59:52.597498Z",
     "shell.execute_reply.started": "2022-03-28T01:59:52.590194Z"
    }
   },
   "outputs": [],
   "source": [
    "example = train_data[0]\n",
    "example.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T01:59:52.600495Z",
     "iopub.status.busy": "2022-03-28T01:59:52.600173Z",
     "iopub.status.idle": "2022-03-28T01:59:52.609952Z",
     "shell.execute_reply": "2022-03-28T01:59:52.609106Z",
     "shell.execute_reply.started": "2022-03-28T01:59:52.600459Z"
    }
   },
   "outputs": [],
   "source": [
    "28**2 *4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T01:59:52.612535Z",
     "iopub.status.busy": "2022-03-28T01:59:52.612275Z",
     "iopub.status.idle": "2022-03-28T01:59:52.620829Z",
     "shell.execute_reply": "2022-03-28T01:59:52.620019Z",
     "shell.execute_reply.started": "2022-03-28T01:59:52.612498Z"
    }
   },
   "outputs": [],
   "source": [
    "reshaped_ex = example.reshape((28,28,4))[:,:,:3] #convert to rgb \n",
    "reshaped_ex.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T01:59:52.622754Z",
     "iopub.status.busy": "2022-03-28T01:59:52.622458Z",
     "iopub.status.idle": "2022-03-28T01:59:52.628203Z",
     "shell.execute_reply": "2022-03-28T01:59:52.627414Z",
     "shell.execute_reply.started": "2022-03-28T01:59:52.622719Z"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T01:59:52.629956Z",
     "iopub.status.busy": "2022-03-28T01:59:52.629567Z",
     "iopub.status.idle": "2022-03-28T01:59:52.842309Z",
     "shell.execute_reply": "2022-03-28T01:59:52.841643Z",
     "shell.execute_reply.started": "2022-03-28T01:59:52.629919Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(reshaped_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T01:59:52.844948Z",
     "iopub.status.busy": "2022-03-28T01:59:52.844451Z",
     "iopub.status.idle": "2022-03-28T01:59:52.850661Z",
     "shell.execute_reply": "2022-03-28T01:59:52.849847Z",
     "shell.execute_reply.started": "2022-03-28T01:59:52.844910Z"
    }
   },
   "outputs": [],
   "source": [
    "ex_label = train_data_label[0]\n",
    "ex_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T01:59:52.852504Z",
     "iopub.status.busy": "2022-03-28T01:59:52.852193Z",
     "iopub.status.idle": "2022-03-28T01:59:53.713529Z",
     "shell.execute_reply": "2022-03-28T01:59:53.712777Z",
     "shell.execute_reply.started": "2022-03-28T01:59:52.852471Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert to RGB for now\n",
    "train_data_reshaped = train_data.reshape(100000,28,28,4)[:,:,:,:3] / 255.\n",
    "test_data_reshaped = test_data.reshape(50000,28,28,4)[:,:,:,:3] / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some models have already been designed for this specific dataset and problem and give good result. Our interest here will be  to compute different types of models and to study their robustnesses to small changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resistance to adversarial attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to measure the robustness of several models to adversarial attacks. \n",
    "\n",
    "In order to do this, we will use 3 different models: a simple convolutionnal net, a convolutionnal model using transfer learning and a model with computer vision features. And we will try 2 types of adversarial attacks: the **Fast Gradient Sign Method** (FGSM) attack and a **black box** attack. \n",
    "\n",
    "We will see later how to measure the robustness of a model to adversarial attacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are adversarial attacks ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For context, there are many categories of adversarial attacks, each with a different goal and assumption of the attacker’s knowledge. However, in general the overarching goal is to add the least amount of perturbation to the input data to cause the desired misclassification. There are several kinds of assumptions of the attacker’s knowledge, two of which are: **white-box** and **black-box**. A white-box attack assumes the attacker has full knowledge and access to the model, including architecture, inputs, outputs, and weights. A black-box attack assumes the attacker only has access to the inputs and outputs of the model, and knows nothing about the underlying architecture or weights. There are also several types of goals, including **misclassification** and **source/target misclassification**. A goal of misclassification means the adversary only wants the output classification to be wrong but does not care what the new classification is. A source/target misclassification means the adversary wants to alter an image that is originally of a specific source class so that it is classified as a specific target class. In this notebook, we will focus on non targeted misclassification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different types of adversarial attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we're interested in two different attacks: the Fast Gradient Sign Method attack and a black box attack. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different models in pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to compare the resistance of different models to adversarial attacks. We test three different models : a simple convolutional network, a network vgg-based trained with transfert learning, and a model with no convolutional layers that uses only other computer vision features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T01:59:53.714989Z",
     "iopub.status.busy": "2022-03-28T01:59:53.714723Z",
     "iopub.status.idle": "2022-03-28T01:59:53.722428Z",
     "shell.execute_reply": "2022-03-28T01:59:53.721735Z",
     "shell.execute_reply.started": "2022-03-28T01:59:53.714955Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T01:59:53.726745Z",
     "iopub.status.busy": "2022-03-28T01:59:53.726474Z",
     "iopub.status.idle": "2022-03-28T01:59:53.734335Z",
     "shell.execute_reply": "2022-03-28T01:59:53.733670Z",
     "shell.execute_reply.started": "2022-03-28T01:59:53.726700Z"
    }
   },
   "outputs": [],
   "source": [
    "class SatImgDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.transform = T.ToTensor()\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = self.transform(self.X[index])\n",
    "        y = torch.FloatTensor(self.y[index])\n",
    "        return {'x':x, 'y':y}\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T01:59:53.736046Z",
     "iopub.status.busy": "2022-03-28T01:59:53.735684Z",
     "iopub.status.idle": "2022-03-28T01:59:54.058272Z",
     "shell.execute_reply": "2022-03-28T01:59:54.057522Z",
     "shell.execute_reply.started": "2022-03-28T01:59:53.736007Z"
    }
   },
   "outputs": [],
   "source": [
    "np.save(\"train_f.npy\",train_data)\n",
    "np.save(\"test_f.npy\",test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T01:59:54.061204Z",
     "iopub.status.busy": "2022-03-28T01:59:54.060665Z",
     "iopub.status.idle": "2022-03-28T01:59:54.067208Z",
     "shell.execute_reply": "2022-03-28T01:59:54.066515Z",
     "shell.execute_reply.started": "2022-03-28T01:59:54.061165Z"
    }
   },
   "outputs": [],
   "source": [
    "np.save(\"train_y.npy\",train_data_label)\n",
    "np.save(\"test_y.npy\",test_data_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T01:59:54.068775Z",
     "iopub.status.busy": "2022-03-28T01:59:54.068504Z",
     "iopub.status.idle": "2022-03-28T01:59:54.081528Z",
     "shell.execute_reply": "2022-03-28T01:59:54.080817Z",
     "shell.execute_reply.started": "2022-03-28T01:59:54.068741Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data_reshaped = train_data.reshape(100000,28,28,4)[:,:,:,:3] \n",
    "test_data_reshaped = test_data.reshape(50000,28,28,4)[:10000,:,:,:3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T01:59:54.083047Z",
     "iopub.status.busy": "2022-03-28T01:59:54.082789Z",
     "iopub.status.idle": "2022-03-28T01:59:54.092010Z",
     "shell.execute_reply": "2022-03-28T01:59:54.091019Z",
     "shell.execute_reply.started": "2022-03-28T01:59:54.083013Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_test = SatImgDataset(test_data_reshaped, test_data_label[:10000])\n",
    "dataset_train = SatImgDataset(train_data_reshaped, train_data_label)\n",
    "\n",
    "loader_train = DataLoader(dataset_train, 512, shuffle=True)\n",
    "loader_test = DataLoader(dataset_test, 512, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple convolutional model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We computed a simple convolutional network that we trained from scratch on a part of the dataset (100000 images). The network is defined as follows:\n",
    "\n",
    "\n",
    "*   Conv2D(32, (3,3), activation='relu', input_shape=(28,28,3))\n",
    "*   MaxPooling2D((2,2)\n",
    "*   Conv2D(64, (3,3), activation='relu')\n",
    "*   MaxPooling2D((2,2))\n",
    "*   Flatten()\n",
    "*   Dense(512, activation='relu')\n",
    "*   Dense(6, activation='softmax')\n",
    "\n",
    "\n",
    "with a cross-entropy loss and an Adam optimizer with learning rate $10e^-4$. This model will give us an accuracy of 96% after 100 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T01:59:54.093540Z",
     "iopub.status.busy": "2022-03-28T01:59:54.093216Z",
     "iopub.status.idle": "2022-03-28T01:59:54.115085Z",
     "shell.execute_reply": "2022-03-28T01:59:54.114364Z",
     "shell.execute_reply.started": "2022-03-28T01:59:54.093506Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 32, (3,3)),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d((2,2)),\n",
    "    nn.Conv2d(32, 64, (3,3)),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d((2,2)),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(1600,512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 6),\n",
    "    nn.Softmax()\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T01:59:54.116372Z",
     "iopub.status.busy": "2022-03-28T01:59:54.116078Z",
     "iopub.status.idle": "2022-03-28T02:03:49.925133Z",
     "shell.execute_reply": "2022-03-28T02:03:49.924430Z",
     "shell.execute_reply.started": "2022-03-28T01:59:54.116337Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "epochs = 30\n",
    "criterion = nn.BCELoss()\n",
    "for e in range(epochs):\n",
    "    for batch in tqdm(loader_train):\n",
    "        pred = model(batch['x'].to(device))\n",
    "        loss = criterion( pred, batch['y'].to(device))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient based Attacks : FGSM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the first and most popular adversarial attacks to date is referred to as the Fast Gradient Sign Attack (FGSM) and is described by Goodfellow et. al. in *Explaining and Harnessing Adversarial Examples* (https://arxiv.org/abs/1412.6572). \n",
    "\n",
    "The attack is remarkably powerful, and yet intuitive. It is designed to attack neural networks by leveraging the way they learn, gradients. The idea is simple, rather than working to minimize the loss by adjusting the weights based on the backpropagated gradients, the attack adjusts the input data to maximize the loss based on the same backpropagated gradients. In other words, the attack uses the gradient of the loss w.r.t the input data, then adjusts the input data to maximize the loss. \n",
    "\n",
    "\n",
    "For instance, let $\\mathbf{x}$ be the original input image, $y$ the ground truth label for $\\mathbf{x}$, $\\mathbf{\\theta}$ the model parameters, and $J(\\mathbf{\\theta}, \\mathbf{x}, y)$ the loss that is used to train the network. The attack backpropagates the gradient back to the input data to calculate $\\nabla_{\\mathbf{x}} J(\\mathbf{\\theta}, \\mathbf{x}, y)$. \n",
    "\n",
    "\n",
    "Then, it adjusts the input data by a small step $\\epsilon$ in the direction that will maximize the loss (i.e. $sign(\\nabla_{\\mathbf{x}} J(\\mathbf{\\theta}, \\mathbf{x}, y))$). The resulting perturbed image, $x'$, is then misclassified by the target network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T02:03:49.926911Z",
     "iopub.status.busy": "2022-03-28T02:03:49.926483Z",
     "iopub.status.idle": "2022-03-28T02:03:49.933408Z",
     "shell.execute_reply": "2022-03-28T02:03:49.932609Z",
     "shell.execute_reply.started": "2022-03-28T02:03:49.926873Z"
    }
   },
   "outputs": [],
   "source": [
    "def fast_gradient_sign_method(model, imgs, labels, attack_params):\n",
    "    # Determine prediction of the model\n",
    "    inp_imgs = imgs.clone().requires_grad_()\n",
    "    preds = model(inp_imgs.to(device))\n",
    "    pred_logprob = torch.log(preds)\n",
    "    #print(labels.dtype)\n",
    "    # Calculate loss by NLL\n",
    "    loss = nn.BCELoss()(preds, labels.to(device))\n",
    "    loss.backward()\n",
    "    # Update image to adversarial example as written above\n",
    "    noise_grad = torch.sign(inp_imgs.grad.to(imgs.device))\n",
    "    fake_imgs = imgs + attack_params['epsilon'] * noise_grad\n",
    "    fake_imgs.detach_()\n",
    "    return fake_imgs, noise_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T02:03:49.935174Z",
     "iopub.status.busy": "2022-03-28T02:03:49.934757Z",
     "iopub.status.idle": "2022-03-28T02:03:49.947363Z",
     "shell.execute_reply": "2022-03-28T02:03:49.946360Z",
     "shell.execute_reply.started": "2022-03-28T02:03:49.935135Z"
    }
   },
   "outputs": [],
   "source": [
    "def eval_model(model, dataloader, fake_generator=None, attack_params=None):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    \n",
    "    for i, batch in enumerate(dataloader):\n",
    "        #print(batch['y'].shape)\n",
    "        if fake_generator is not None:\n",
    "            fake_imgs, _ = fake_generator(model, batch['x'],batch['y'], attack_params)\n",
    "            with torch.no_grad():\n",
    "                pred = model(fake_imgs.to(device))\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                pred = model(batch['x'].to(device))\n",
    "        y_pred.append(pred.argmax(dim=1).cpu().numpy())\n",
    "        y_true.append(batch['y'].argmax(dim=1).numpy())\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    y_true = np.concatenate(y_true)\n",
    "    return accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T02:03:49.948983Z",
     "iopub.status.busy": "2022-03-28T02:03:49.948703Z",
     "iopub.status.idle": "2022-03-28T02:03:57.148302Z",
     "shell.execute_reply": "2022-03-28T02:03:57.147634Z",
     "shell.execute_reply.started": "2022-03-28T02:03:49.948936Z"
    }
   },
   "outputs": [],
   "source": [
    "eps = np.logspace(-2, 0, 10)\n",
    "acc_attack = [eval_model(model, loader_test, fast_gradient_sign_method, {'epsilon': e}) for e in eps]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T02:03:57.149946Z",
     "iopub.status.busy": "2022-03-28T02:03:57.149702Z",
     "iopub.status.idle": "2022-03-28T02:03:57.638165Z",
     "shell.execute_reply": "2022-03-28T02:03:57.637492Z",
     "shell.execute_reply.started": "2022-03-28T02:03:57.149913Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(eps, acc_attack)\n",
    "plt.xscale('log')\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel('epsilon')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T02:03:57.639756Z",
     "iopub.status.busy": "2022-03-28T02:03:57.639478Z",
     "iopub.status.idle": "2022-03-28T02:03:57.646155Z",
     "shell.execute_reply": "2022-03-28T02:03:57.645504Z",
     "shell.execute_reply.started": "2022-03-28T02:03:57.639721Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_one_image(img_data, y):\n",
    "    with torch.no_grad():\n",
    "        pred = model(T.ToTensor()(img_data).unsqueeze(0).to(device))\n",
    "    true_label = y.argmax()\n",
    "    pred_label = pred[0].argmax()\n",
    "    print(pred)\n",
    "    plt.imshow(img_data)\n",
    "    print(\"True class: {}, predicted class: {}\".format(true_label, pred_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T02:03:57.647805Z",
     "iopub.status.busy": "2022-03-28T02:03:57.647534Z",
     "iopub.status.idle": "2022-03-28T02:03:57.667374Z",
     "shell.execute_reply": "2022-03-28T02:03:57.666600Z",
     "shell.execute_reply.started": "2022-03-28T02:03:57.647766Z"
    }
   },
   "outputs": [],
   "source": [
    "attack_params = {'epsilon':0.03}\n",
    "\n",
    "img_data = test_data[223].reshape((28,28,4))[:,:,:3]\n",
    "labels = torch.FloatTensor(test_data_label[223]).unsqueeze(0)\n",
    "\n",
    "f_img, noise_grad = fast_gradient_sign_method(model, T.ToTensor()(img_data).unsqueeze(0), labels, attack_params)\n",
    "fake_img = f_img.squeeze().numpy().transpose(1,2,0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T02:03:57.668503Z",
     "iopub.status.busy": "2022-03-28T02:03:57.668318Z",
     "iopub.status.idle": "2022-03-28T02:03:57.864807Z",
     "shell.execute_reply": "2022-03-28T02:03:57.864116Z",
     "shell.execute_reply.started": "2022-03-28T02:03:57.668481Z"
    }
   },
   "outputs": [],
   "source": [
    "predict_one_image(img_data, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T02:03:57.866635Z",
     "iopub.status.busy": "2022-03-28T02:03:57.866323Z",
     "iopub.status.idle": "2022-03-28T02:03:58.043216Z",
     "shell.execute_reply": "2022-03-28T02:03:58.042457Z",
     "shell.execute_reply.started": "2022-03-28T02:03:57.866538Z"
    }
   },
   "outputs": [],
   "source": [
    "predict_one_image(fake_img, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Black box attack : SIMBA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will be particularly useful for the model with computer vision features, for which we do not have access to gradients, thus we can't use white box attacks. Indeed, black-box attacks require only queries to the target model that may return complete or partial information. The algorithm we use (described in https://arxiv.org/abs/1905.07121) uses the following simple iterative principle: we randomly sample a vector from a predefined orthonormal basis and either add or subtract it to the target image.\n",
    "\n",
    "We assume we have some image $\\mathbf{x}$ which a black-box neural network, $h$, classifies $h(\\mathbf{x}) = y$ with predicted confidence or output probability $p_h(y | \\mathbf{x})$. The goal is to find a small perturbation $\\delta$ such that the prediction $h(\\mathbf{x} + \\delta) = y$. \n",
    "\n",
    "Although gradient information is absent in the black-box setting, we argue that the presence of output probabilities can serve as a strong proxy to guide the search for adversarial images. The intuition behind the method is simple: for any direction $\\mathbf{q}$ and some step size $\\epsilon$, one of $\\mathbf{x} + \\epsilon \\mathbf{q}$ or $\\mathbf{x} − \\epsilon \\mathbf{q}$ is likely to decrease $p_h(y | \\mathbf{x})$. \n",
    "\n",
    "We therefore repeatedly pick random directions $\\mathbf{q}$ and either add or subtract them. To minimize the number of queries to $h(·)$ we always first try adding $\\mathbf{q}$. If this decreases the probability $p_h(y | \\mathbf{x})$ we take the step, otherwise we try subtracting $\\mathbf{q}$. This procedure requires between $1.4$ and $1.5$ queries per update on average (depending on the data set and target model). \n",
    "\n",
    "The method – Simple Black-box Attack (SimBA) – takes as input the target image label pair $(\\mathbf{x},y)$, a set of orthonormal candidate vectors $\\mathcal{Q}$ and a step-size $\\epsilon > 0$. For simplicity we pick $\\mathbf{q} \\in \\mathcal{Q}$ uniformly at random. To guarantee maximum query efficiency, we ensure that no two directions cancel each other out and diminish progress, or amplify each other and increase the norm of $\\delta$ disproportionately. For this reason we pick $\\mathbf{q}$ without replacement and restrict all vectors in $\\mathcal{Q}$ to be orthonormal. The only hyper-parameters of SimBA are the set of orthogonal search vectors $\\mathcal{Q}$ and the step size $\\epsilon$.\n",
    "\n",
    "\n",
    "The parameters $\\epsilon$ will help us quantify how robust our models are. The bigger $\\epsilon$ needs to be in order to get images misclassified, the more robust our models are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T02:03:58.044518Z",
     "iopub.status.busy": "2022-03-28T02:03:58.044286Z",
     "iopub.status.idle": "2022-03-28T02:03:58.056034Z",
     "shell.execute_reply": "2022-03-28T02:03:58.055344Z",
     "shell.execute_reply.started": "2022-03-28T02:03:58.044481Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_probs(model, x, y):\n",
    "    output = model(x)\n",
    "    probs = torch.gather(output, 1, y.unsqueeze(1)).squeeze()\n",
    "    return probs\n",
    "\n",
    "def simba_single(model, img, y, num_iters=1000, epsilon=0.05):\n",
    "    x = img.clone()\n",
    "    n_dims = x.view(x.size(0), -1).size(1)\n",
    "    perm = torch.randperm(n_dims)\n",
    "    #print(x.view(x.size(1), -1).shape)\n",
    "    last_prob = get_probs(model, x, y)\n",
    "    for i in range(num_iters):\n",
    "        diff = torch.zeros(x.shape[0], n_dims).to(device)\n",
    "        diff[:, perm[i]] = epsilon\n",
    "        with torch.no_grad():\n",
    "            left_prob = get_probs(model, (x - diff.view(x.size())).clamp(0, 1), y)\n",
    "            right_prob = get_probs(model, (x + diff.view(x.size())).clamp(0, 1), y)\n",
    "        \n",
    "        left_mask = left_prob < last_prob\n",
    "        right_mask = right_prob < last_prob\n",
    "        \n",
    "        \n",
    "        x[left_mask] = (x[left_mask] - diff[left_mask].view(x[left_mask].size())).clamp(0, 1)\n",
    "        x[right_mask] = (x[right_mask] + diff[right_mask].view(x[right_mask].size())).clamp(0, 1)\n",
    "        \n",
    "        last_prob[left_mask] = left_prob[left_mask]\n",
    "        last_prob[right_mask] = right_prob[right_mask]\n",
    "\n",
    "\n",
    "    return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T02:03:58.057700Z",
     "iopub.status.busy": "2022-03-28T02:03:58.057253Z",
     "iopub.status.idle": "2022-03-28T02:25:21.489529Z",
     "shell.execute_reply": "2022-03-28T02:25:21.488858Z",
     "shell.execute_reply.started": "2022-03-28T02:03:58.057666Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "eps = np.logspace(-2, 0, 10)\n",
    "acc_attack = []\n",
    "fake=True\n",
    "for e in eps:\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    for i, batch in enumerate(tqdm(loader_test)):\n",
    "        #print(batch['y'].shape)\n",
    "        if fake:\n",
    "            fake_imgs = simba_single(model, \n",
    "                                     batch['x'].to(device) ,\n",
    "                                     batch['y'].argmax(dim=1).to(device),\n",
    "                                    epsilon=e)\n",
    "            with torch.no_grad():\n",
    "                pred = model(fake_imgs.to(device))\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                pred = model(batch['x'].to(device))\n",
    "        y_pred.append(pred.argmax(dim=1).cpu().numpy())\n",
    "        y_true.append(batch['y'].argmax(dim=1).numpy())\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    y_true = np.concatenate(y_true)\n",
    "    acc_attack.append(accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T02:25:21.491089Z",
     "iopub.status.busy": "2022-03-28T02:25:21.490779Z",
     "iopub.status.idle": "2022-03-28T02:25:21.927153Z",
     "shell.execute_reply": "2022-03-28T02:25:21.926442Z",
     "shell.execute_reply.started": "2022-03-28T02:25:21.491054Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(eps, acc_attack)\n",
    "plt.xscale('log')\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel('epsilon')\n",
    "plt.savefig(\"cnn_simba.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T02:25:21.932595Z",
     "iopub.status.busy": "2022-03-28T02:25:21.931959Z",
     "iopub.status.idle": "2022-03-28T02:25:22.699828Z",
     "shell.execute_reply": "2022-03-28T02:25:22.699096Z",
     "shell.execute_reply.started": "2022-03-28T02:25:21.932545Z"
    }
   },
   "outputs": [],
   "source": [
    "img_data = test_data[12].reshape((28,28,4))[:,:,:3]\n",
    "labels = torch.FloatTensor(test_data_label[12]).unsqueeze(0).to(device)\n",
    "\n",
    "f_img = simba_single(model, T.ToTensor()(img_data).unsqueeze(0).to(device), labels.argmax(1), num_iters=500)\n",
    "fake_img = f_img.squeeze().cpu().numpy().transpose(1,2,0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T02:25:22.701297Z",
     "iopub.status.busy": "2022-03-28T02:25:22.700966Z",
     "iopub.status.idle": "2022-03-28T02:25:22.878494Z",
     "shell.execute_reply": "2022-03-28T02:25:22.877771Z",
     "shell.execute_reply.started": "2022-03-28T02:25:22.701260Z"
    }
   },
   "outputs": [],
   "source": [
    "predict_one_image(fake_img, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T02:25:22.880773Z",
     "iopub.status.busy": "2022-03-28T02:25:22.879813Z",
     "iopub.status.idle": "2022-03-28T02:25:23.062721Z",
     "shell.execute_reply": "2022-03-28T02:25:23.062018Z",
     "shell.execute_reply.started": "2022-03-28T02:25:22.880734Z"
    }
   },
   "outputs": [],
   "source": [
    "predict_one_image(img_data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with transfert learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a big dataset with images to classify, so we try to do tranfert learning to benefit from pre-training of models on other datasets.\n",
    "\n",
    "The main problems concerning transfert learning is that:\n",
    "\n",
    "\n",
    "*   The datasets on which pretrained models are trained are very different from satellite images, so the features that are learned may not be relevant for our problem. This is why we looked for models trained on satellite images. We found some pretrained models such as BigEarthNet, but the minimal input size was ten times the size of our images, so we did not use these models.\n",
    "\n",
    "*   The images at our disposal are very small (28x28), whereas pretrained models take inputs of at least 32x32, since it is the minimal dimension of the network. We solved this problem by using the resize function from open-cv that allows to upscale an image.\n",
    "\n",
    "We used VGG-16 where we replaced the last layer by a dense layer of output size 6 (the number of classes). We froze all VGG layers except the final one and we trained the model. \n",
    "\n",
    "We got a score close to the score from our first CNN, but this model was ten times longer to train. The score was not increased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T02:25:23.064245Z",
     "iopub.status.busy": "2022-03-28T02:25:23.064000Z",
     "iopub.status.idle": "2022-03-28T02:25:23.069382Z",
     "shell.execute_reply": "2022-03-28T02:25:23.068471Z",
     "shell.execute_reply.started": "2022-03-28T02:25:23.064212Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from cv2 import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T02:25:23.071414Z",
     "iopub.status.busy": "2022-03-28T02:25:23.070883Z",
     "iopub.status.idle": "2022-03-28T02:25:24.058057Z",
     "shell.execute_reply": "2022-03-28T02:25:24.057323Z",
     "shell.execute_reply.started": "2022-03-28T02:25:23.071374Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data_reshaped_big = []\n",
    "test_data_reshaped_big = []\n",
    "\n",
    "for i in range(len(train_data_reshaped[:50000])):\n",
    "    train_data_reshaped_big.append(resize(train_data_reshaped[i], (32,32)))\n",
    "    \n",
    "for i in range(len(test_data_reshaped[:25000])):\n",
    "    test_data_reshaped_big.append(resize(test_data_reshaped[i], (32,32)))\n",
    "    \n",
    "train_data_reshaped_big, test_data_reshaped_big = np.array(train_data_reshaped_big), np.array(test_data_reshaped_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T02:37:32.325435Z",
     "iopub.status.busy": "2022-03-28T02:37:32.325170Z",
     "iopub.status.idle": "2022-03-28T02:37:32.332025Z",
     "shell.execute_reply": "2022-03-28T02:37:32.329559Z",
     "shell.execute_reply.started": "2022-03-28T02:37:32.325407Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data_reshaped_big.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T02:37:45.327471Z",
     "iopub.status.busy": "2022-03-28T02:37:45.327222Z",
     "iopub.status.idle": "2022-03-28T02:37:45.333194Z",
     "shell.execute_reply": "2022-03-28T02:37:45.332471Z",
     "shell.execute_reply.started": "2022-03-28T02:37:45.327443Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_test = SatImgDataset(test_data_reshaped_big, test_data_label[:50000])\n",
    "dataset_train = SatImgDataset(train_data_reshaped_big, train_data_label[:25000])\n",
    "\n",
    "loader_train = DataLoader(dataset_train, 512, shuffle=True)\n",
    "loader_test = DataLoader(dataset_test, 512, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T02:37:46.683830Z",
     "iopub.status.busy": "2022-03-28T02:37:46.683153Z",
     "iopub.status.idle": "2022-03-28T02:37:48.290382Z",
     "shell.execute_reply": "2022-03-28T02:37:48.289666Z",
     "shell.execute_reply.started": "2022-03-28T02:37:46.683793Z"
    }
   },
   "outputs": [],
   "source": [
    "model_pretrained = models.vgg16(pretrained=True)\n",
    "\n",
    "for param in model_pretrained.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "number_features = model_pretrained.classifier[6].in_features\n",
    "features = list(model_pretrained.classifier.children())[:-1] # Remove last layer\n",
    "features.extend([torch.nn.Linear(number_features, 6)])\n",
    "model_pretrained.classifier = torch.nn.Sequential(*features)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer_ft = optim.SGD(model_pretrained.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "model_pretrained = model_pretrained.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T02:37:48.631341Z",
     "iopub.status.busy": "2022-03-28T02:37:48.630886Z",
     "iopub.status.idle": "2022-03-28T02:37:48.675470Z",
     "shell.execute_reply": "2022-03-28T02:37:48.674324Z",
     "shell.execute_reply.started": "2022-03-28T02:37:48.631304Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "for e in range(epochs):\n",
    "    for batch in tqdm(loader_train):\n",
    "        pred = model_pretrained(batch['x'].float().to(device))\n",
    "        loss = criterion( pred, batch['y'].to(device))\n",
    "        optimizer_ft.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_ft.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-03-28T02:26:02.413948Z",
     "iopub.status.idle": "2022-03-28T02:26:02.414425Z",
     "shell.execute_reply": "2022-03-28T02:26:02.414217Z",
     "shell.execute_reply.started": "2022-03-28T02:26:02.414194Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy : \",eval_model(model_pretrained, loader_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-03-28T02:26:02.416004Z",
     "iopub.status.idle": "2022-03-28T02:26:02.416641Z",
     "shell.execute_reply": "2022-03-28T02:26:02.416411Z",
     "shell.execute_reply.started": "2022-03-28T02:26:02.416387Z"
    }
   },
   "outputs": [],
   "source": [
    "eps = np.logspace(-3, -1, 10)\n",
    "acc_attack = [eval_model(model_pretrained, loader_test, fast_gradient_sign_method, {'epsilon': e}) for e in eps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-03-28T02:26:02.417870Z",
     "iopub.status.idle": "2022-03-28T02:26:02.418488Z",
     "shell.execute_reply": "2022-03-28T02:26:02.418282Z",
     "shell.execute_reply.started": "2022-03-28T02:26:02.418259Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(eps, acc_attack)\n",
    "plt.xscale('log')\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel('epsilon')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with computer vision features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did not compute this model, it is public on the Kaggle page of the Deepsat dataset, it was created by Rahul KUMAR.\n",
    "\n",
    "The particularities of this model is that it does not use convolutions to extract features from the images, but computer vision features. We detail below these features:\n",
    "\n",
    "\n",
    "\n",
    "*   texture features\n",
    "*   NIR : near infrared channel\n",
    "*   HSV features\n",
    "*   NDVI features\n",
    "*   ARVI\n",
    "\n",
    "This gives 17 features in total per image.\n",
    "\n",
    "Then, these vectors of features are passed through a network composed of these three layers:\n",
    "* Dense(50), Activation(\"relu\"), Dropout(0.2))\n",
    "* Dense(50), Activation(\"relu\"), Dropout(0.2)\n",
    "* Dense(6, activation=\"softmax\")\n",
    "\n",
    "After 500 epochs, the score is 99.38%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T02:38:09.203343Z",
     "iopub.status.busy": "2022-03-28T02:38:09.202839Z",
     "iopub.status.idle": "2022-03-28T02:38:26.944207Z",
     "shell.execute_reply": "2022-03-28T02:38:26.943332Z",
     "shell.execute_reply.started": "2022-03-28T02:38:09.203306Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from skimage import color\n",
    "!pip install mahotas\n",
    "import mahotas as mt\n",
    "from skimage.color import rgb2gray\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T02:38:26.946866Z",
     "iopub.status.busy": "2022-03-28T02:38:26.946586Z",
     "iopub.status.idle": "2022-03-28T02:38:26.959100Z",
     "shell.execute_reply": "2022-03-28T02:38:26.958397Z",
     "shell.execute_reply.started": "2022-03-28T02:38:26.946833Z"
    }
   },
   "outputs": [],
   "source": [
    "def feature_extractor(data):\n",
    "    \n",
    "        tex_feature=[]\n",
    "        hsv_feature=[]\n",
    "        ndvi_feature=[]\n",
    "        arvi_feature=[]\n",
    "\n",
    "        #for df_chunk in tqdm(pd.read_csv(input_image_file ,header=None,chunksize = 5000)):\n",
    "            \n",
    "        #df_chunk=df_chunk.astype(\"int32\")\n",
    "        #data=df_chunk.values\n",
    "\n",
    "\n",
    "        ################data for HSV and Texture feature##############\n",
    "        img=data.reshape(-1,28,28,4)[:,:,:,:3]\n",
    "        #############################################################\n",
    "\n",
    "        ######################Data for NDVI and ARVI#################\n",
    "\n",
    "        NIR=data.reshape(-1,28,28,4)[:,:,:,3]\n",
    "        Red=data.reshape(-1,28,28,4)[:,:,:,2]\n",
    "        Blue=data.reshape(-1,28,28,4)[:,:,:,0]\n",
    "        #############################################################\n",
    "\n",
    "        for i in range(len(data)):\n",
    "\n",
    "            #######Texture_feature####################################\n",
    "            textures = mt.features.haralick(img[i])\n",
    "            ht_mean= textures.mean(axis=0)\n",
    "            tex_feature.append(ht_mean)\n",
    "            ##########################################################\n",
    "\n",
    "            #######hsv_feature#########################################\n",
    "            img_hsv = color.rgb2hsv(img[i]) # Image into HSV colorspace\n",
    "            h = img_hsv[:,:,0] # Hue\n",
    "            s = img_hsv[:,:,1] # Saturation\n",
    "            v = img_hsv[:,:,2] # Value aka Lightness\n",
    "            hsv_feature.append((h.mean(),s.mean(),v.mean()))\n",
    "            ###########################################################\n",
    "\n",
    "            ##########Calculation of NDVI Feature######################\n",
    "            NDVI=(NIR[i]-Red[i])/(NIR[i]+Red[i])\n",
    "            ndvi_feature.append(NDVI.mean())\n",
    "            ############################################################\n",
    "\n",
    "            ###################Calculation of ARVI#####################\n",
    "            a_1=NIR[i] -(2*Red[i]-Blue[i])\n",
    "            a_2=NIR[i] +(2*Red[i]+Blue[i])\n",
    "            arvi=a_1/a_2\n",
    "            arvi_feature.append(arvi.mean())\n",
    "            #######################################################\n",
    "\n",
    "        features=[]\n",
    "        for i in range(len(tex_feature)):\n",
    "            h_stack=np.hstack((tex_feature[i], hsv_feature[i], ndvi_feature[i], arvi_feature[i]))\n",
    "            features.append(h_stack)\n",
    "\n",
    "        return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T02:38:26.960927Z",
     "iopub.status.busy": "2022-03-28T02:38:26.960409Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data_features=feature_extractor(train_data[:50000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save( \"train_cv.npy\",train_data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"test_cv.npy\", test_data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_features=feature_extractor(test_data[:20000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_data_features.shape[1]==18:\n",
    "    train_data_features_r = train_data_features[:,:-2]\n",
    "    test_data_features_r = test_data_features[:,:-2]\n",
    "sc=StandardScaler()\n",
    "#fit the training data\n",
    "fit=sc.fit(train_data_features)\n",
    "\n",
    "##transform the train and test data\n",
    "train_data_stn=fit.transform(train_data_features)\n",
    "test_data_stn=fit.transform(test_data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = nn.Sequential(\n",
    "    nn.Linear(16, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(50, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(50,6),\n",
    "    nn.Softmax()\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SatImgDatasetCV(Dataset):\n",
    "    def __init__(self, X, y, images):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.images = images\n",
    "        self.transform = T.ToTensor()\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img = self.transform(self.images[index])\n",
    "        y = torch.FloatTensor(self.y[index])\n",
    "        x = torch.FloatTensor(self.X[index])\n",
    "        return {'x':x, 'y':y, 'img': img}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_reshaped = train_data.reshape(100000,28,28,4)[:,:,:,:3] \n",
    "test_data_reshaped = test_data.reshape(50000,28,28,4)[:,:,:,:3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test_f = SatImgDatasetCV(test_data_stn, test_data_label[:20000], test_data.reshape(-1,28,28,4))\n",
    "dataset_train_f = SatImgDatasetCV(train_data_stn, train_data_label[:50000],train_data.reshape(-1,28,28,4))\n",
    "\n",
    "loader_train_f = DataLoader(dataset_train_f, 5000, shuffle=True)\n",
    "loader_test_f = DataLoader(dataset_test_f, 512, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model.train()\n",
    "optimizer = torch.optim.Adam(mlp_model.parameters(), lr=1e-3)\n",
    "epochs = 50\n",
    "criterion = nn.BCELoss()\n",
    "for e in range(epochs):\n",
    "    for batch in tqdm(loader_train_f):\n",
    "        pred = mlp_model(batch['x'].to(device))\n",
    "        loss = criterion( pred, batch['y'].to(device))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model.eval()\n",
    "pred_test = []\n",
    "y_test = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(loader_test_f):\n",
    "        pred = mlp_model(batch['x'].to(device))\n",
    "        pred_test.append(pred.argmax(1).cpu().squeeze().numpy())\n",
    "        y_test.append(batch['y'].argmax(1).cpu().squeeze().numpy())\n",
    "        \n",
    "pred_test = np.concatenate(pred_test)\n",
    "y_test = np.concatenate(y_test)\n",
    "print(\"accuracy_score = {}\".format(accuracy_score(y_test, pred_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probs(model, x, y):\n",
    "    output = model(x)\n",
    "   # print(output)\n",
    "    probs = torch.gather(output, 1, y.unsqueeze(1)).squeeze()\n",
    "    return probs\n",
    "\n",
    "def simba_single_mlp(model, img, y, num_iters=1000, epsilon=0.05):\n",
    "    x = img.clone()\n",
    "    \n",
    "    n_dims = x.view(x.size(0), -1).size(1)\n",
    "    perm = torch.randperm(n_dims)\n",
    "    \n",
    "    x_f = (x.permute(0,2,3,1).reshape(x.size(0),-1).cpu().detach().numpy()*255).astype(np.uint8)\n",
    "   # print(x_f.shape)\n",
    "    x_f = fit.transform(feature_extractor(x_f)[:,:-2])\n",
    "\n",
    "    last_prob = get_probs(model, torch.FloatTensor(x_f).to(device), y)\n",
    "    for i in tqdm(range(num_iters)):\n",
    "        \n",
    "        diff = torch.zeros(x.shape[0], n_dims).to(device)\n",
    "        diff[:, perm[i]] = epsilon\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            x_left = (x - diff.view(x.size())).clamp(0, 1)\n",
    "            x_right = (x + diff.view(x.size())).clamp(0, 1)\n",
    "\n",
    "            x_f = feature_extractor((x.permute(0,2,3,1).reshape(x.size(0),-1).cpu().detach().numpy()*255).astype(np.uint8))[:,:-2]\n",
    "            x_l_f = feature_extractor((x_left.permute(0,2,3,1).reshape(x.size(0),-1).cpu().detach().numpy()*255).astype(np.uint8))[:,:-2]\n",
    "            x_r_f = feature_extractor((x_right.permute(0,2,3,1).reshape(x.size(0),-1).cpu().detach().numpy()*255).astype(np.uint8))[:,:-2]\n",
    "            left_prob = get_probs(model, torch.FloatTensor(fit.transform(x_l_f)).to(device), y)\n",
    "            right_prob = get_probs(model, torch.FloatTensor(fit.transform(x_r_f)).to(device), y)\n",
    "\n",
    "        left_mask = left_prob < last_prob\n",
    "        right_mask = right_prob < last_prob\n",
    "        \n",
    "        x[left_mask] = x_left[left_mask]\n",
    "    \n",
    "        x[right_mask] = x_right[right_mask]\n",
    "        \n",
    "        last_prob[left_mask] = left_prob[left_mask]\n",
    "        last_prob[right_mask] = right_prob[right_mask]\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data = test_data[12].reshape((28,28,4))\n",
    "labels = torch.FloatTensor(test_data_label[12]).unsqueeze(0).to(device)\n",
    "\n",
    "f_img = simba_single_mlp(mlp_model, T.ToTensor()(img_data).unsqueeze(0).to(device), labels.argmax(1), num_iters=70, epsilon=0.3)\n",
    "fake_img = f_img.squeeze().cpu().numpy().transpose(1,2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_one_image_mlp(img_data, y):\n",
    "    #print(\"ok\", img_data.reshape(1,-1).mean())\n",
    "    with torch.no_grad():\n",
    "     #   print(feature_extractor(img_data.reshape(1,-1))[:,:-2])\n",
    "        x_f = fit.transform(feature_extractor(img_data.reshape(1,-1))[:,:-2])\n",
    "      #  print(x_f)\n",
    "        pred = mlp_model(torch.FloatTensor(x_f).to(device))\n",
    "    true_label = y.argmax()\n",
    "    pred_label = pred[0].argmax()\n",
    "    print(pred)\n",
    "    plt.imshow(img_data[:,:,:3])\n",
    "    print(\"True class: {}, predicted class: {}\".format(true_label, pred_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_one_image_mlp(img_data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_one_image_mlp((fake_img * 255).astype(np.uint8), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = [0.05, 0.2, 0.5]\n",
    "acc_attack = []\n",
    "\n",
    "for e in eps:\n",
    "    for i in range(20):\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        #print(batch['y'].shape)\n",
    "        f_img = simba_single_mlp(mlp_model, dataset_test_f[i]['img'].unsqueeze(0).to(device), \n",
    "                                 dataset_test_f[i]['y'].argmax().unsqueeze(0).to(device), \n",
    "                                 num_iters=500,\n",
    "                                 epsilon=0.3)\n",
    "        fake_img = (f_img.squeeze().cpu().numpy().transpose(1,2,0) * 255).astype(np.uint8)\n",
    "        x_f = fit.transform(feature_extractor(fake_img.reshape(1,-1))[:,:-2])\n",
    "        with torch.no_grad():\n",
    "            pred = mlp_model(torch.FloatTensor(x_f).to(device))\n",
    "\n",
    "\n",
    "        y_pred.append(pred.argmax().cpu().numpy())\n",
    "        y_true.append(dataset_test_f[i]['y'].argmax().numpy())\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_true = np.array(y_true)\n",
    "    acc_attack.append(accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(eps, acc_attack)\n",
    "plt.xscale('log')\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel('epsilon')\n",
    "plt.savefig(\"cvf_simba.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annex : Keras models, training and scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), activation='relu', input_shape=(28,28,3)))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(6, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', \n",
    "             optimizer=optimizers.RMSprop(learning_rate=1e-4),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_data_reshaped, train_data_label, validation_split=0.20, batch_size=512, epochs=100, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values)+1)\n",
    "\n",
    "plt.plot(epochs, loss_values, 'bo', label='Entraînement')\n",
    "plt.plot(epochs, val_loss_values, 'b', label = 'Validation')\n",
    "\n",
    "plt.title('Perte pdt l\\'entraînement et la validation')\n",
    "plt.xlabel(\"Nombre d'époques\")\n",
    "plt.ylabel('Perte')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_values = history_dict['acc']\n",
    "val_acc_values = history_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'bo', label='Entraînement')\n",
    "plt.plot(epochs, val_acc_values, 'b', label = 'Validation')\n",
    "\n",
    "plt.title('Exactitude pdt l\\'entraînement et la validation')\n",
    "plt.xlabel(\"Nombre d'époques\")\n",
    "plt.ylabel('Exactitude de prediction')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = model.evaluate(test_data_reshaped, test_data_label)[1]\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.math import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_label_pred = model.predict(test_data_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(test_data_label.argmax(axis=1), test_data_label_pred.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfert learning with VGG-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from cv2 import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_reshaped_big = []\n",
    "test_data_reshaped_big = []\n",
    "\n",
    "for i in range(len(train_data_reshaped)):\n",
    "    train_data_reshaped_big.append(resize(train_data_reshaped[i], (32,32)))\n",
    "    \n",
    "for i in range(len(test_data_reshaped)):\n",
    "    test_data_reshaped_big.append(resize(test_data_reshaped[i], (32,32)))\n",
    "    \n",
    "train_data_reshaped_big, test_data_reshaped_big = np.array(train_data_reshaped_big), np.array(test_data_reshaped_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_model = Sequential()\n",
    "\n",
    "pretrained_model= VGG16(include_top=False,\n",
    "                   input_shape=(32,32,3),\n",
    "                   pooling='avg',classes=6,\n",
    "                   weights='imagenet')\n",
    "for layer in pretrained_model.layers:\n",
    "        layer.trainable=False\n",
    "\n",
    "vgg_model.add(pretrained_model)\n",
    "\n",
    "vgg_model.add(Flatten())\n",
    "vgg_model.add(Dense(256, activation='relu'))\n",
    "vgg_model.add(Dense(6, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=Adam(learning_rate=0.001),\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = vgg_model.fit(train_data_reshaped_big, train_data_label, validation_split=0.20, batch_size=512, epochs=3, callbacks=[es])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
